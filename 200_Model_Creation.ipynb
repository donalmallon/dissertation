{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200_Model_Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries must be installed before importing \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/donalmallon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## download nltk wordnet and spacy and wordnet\n",
    "nltk.download('wordnet')\n",
    "spacy.load('en')\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 201: Methods to prepare/preprocess text for lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/donalmallon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## download all english stopwords \n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 202: Load in petitions info and other transcripts \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "petitions_info = pd.read_pickle(\"petitions_info.pickle\")\n",
    "other_tscripts= pd.read_pickle(\"other_transcripts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20961, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider only petitions that have recieved over 20 signatures \n",
    "mask = petitions_info['Signatures Count'] >20\n",
    "petitions_info = petitions_info.loc[mask].reset_index(drop = True)\n",
    "petitions_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 203: Create universal dictionary from the text both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess other transcripts text data for LDA \n",
    "#//2 mins//#\n",
    "petitions_txt= []\n",
    "len(petitions_info)\n",
    "for x in range(0, len(petitions_info.information)):\n",
    "    if x%10000== 0:\n",
    "            print(x)\n",
    "    petitions_txt.append(prepare_text_for_lda(str(petitions_info.information[x]) + str(petitions_info.more_info[x])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess other transcripts text data for LDA \n",
    "#//~3 hours//#\n",
    "\n",
    "tscript_txt= []\n",
    "\n",
    "for x in range(0, len(other_tscripts)):\n",
    "    if x%1000== 0:\n",
    "            print(x)\n",
    "    tscript_txt.append(prepare_text_for_lda(str(other_tscripts.tscript[x])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(petitions_txt+tscript_txt)\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 204 Create LDA model for petitions data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create corpus based on universal dictionary and save \n",
    "petitions_corpus = [dictionary.doc2bow(text) for text in petitions_txt]\n",
    "pickle.dump(petitions_corpus, open('petitions_corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.714300870895386"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numTopics = 20\n",
    "t = time.time() ##time process\n",
    "petitions_ldamodel = LdaMulticore(petitions_corpus, num_topics = numTopics, id2word=dictionary)\n",
    "petitions_ldamodel.save('petitions_model.gensim')\n",
    "elapsed = time.time() - t\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.008*\"people\" + 0.007*\"government\" + 0.006*\"service\" + 0.006*\"country\" + 0.005*\"british\"')\n",
      "(1, '0.007*\"government\" + 0.007*\"would\" + 0.005*\"people\" + 0.005*\"petition\" + 0.004*\"crime\"')\n",
      "(2, '0.008*\"police\" + 0.008*\"people\" + 0.008*\"driver\" + 0.007*\"government\" + 0.004*\"pension\"')\n",
      "(3, '0.022*\"school\" + 0.011*\"would\" + 0.010*\"child\" + 0.008*\"petition\" + 0.006*\"education\"')\n",
      "(4, '0.005*\"government\" + 0.004*\"people\" + 0.004*\"child\" + 0.004*\"petition\" + 0.003*\"would\"')\n",
      "(5, '0.018*\"child\" + 0.018*\"people\" + 0.009*\"parent\" + 0.007*\"government\" + 0.007*\"would\"')\n",
      "(6, '0.011*\"people\" + 0.008*\"government\" + 0.008*\"animal\" + 0.007*\"child\" + 0.006*\"health\"')\n",
      "(7, '0.009*\"people\" + 0.007*\"government\" + 0.007*\"health\" + 0.006*\"year\" + 0.006*\"patient\"')\n",
      "(8, '0.027*\"child\" + 0.013*\"government\" + 0.007*\"parent\" + 0.005*\"people\" + 0.005*\"petition\"')\n",
      "(9, '0.011*\"government\" + 0.008*\"vehicle\" + 0.005*\"child\" + 0.004*\"would\" + 0.004*\"public\"')\n",
      "(10, '0.005*\"people\" + 0.005*\"petition\" + 0.004*\"school\" + 0.003*\"government\" + 0.003*\"year\"')\n",
      "(11, '0.006*\"service\" + 0.006*\"plastic\" + 0.004*\"year\" + 0.004*\"people\" + 0.004*\"government\"')\n",
      "(12, '0.004*\"service\" + 0.004*\"public\" + 0.004*\"people\" + 0.004*\"right\" + 0.003*\"animal\"')\n",
      "(13, '0.013*\"government\" + 0.010*\"british\" + 0.010*\"right\" + 0.009*\"child\" + 0.006*\"would\"')\n",
      "(14, '0.011*\"government\" + 0.007*\"animal\" + 0.007*\"people\" + 0.005*\"would\" + 0.005*\"election\"')\n",
      "(15, '0.015*\"people\" + 0.006*\"government\" + 0.005*\"country\" + 0.005*\"animal\" + 0.004*\"public\"')\n",
      "(16, '0.008*\"would\" + 0.008*\"people\" + 0.007*\"government\" + 0.005*\"could\" + 0.005*\"child\"')\n",
      "(17, '0.017*\"people\" + 0.010*\"government\" + 0.009*\"health\" + 0.009*\"would\" + 0.008*\"mental\"')\n",
      "(18, '0.015*\"people\" + 0.012*\"child\" + 0.011*\"government\" + 0.007*\"service\" + 0.006*\"would\"')\n",
      "(19, '0.013*\"government\" + 0.009*\"public\" + 0.007*\"would\" + 0.006*\"student\" + 0.005*\"petition\"')\n"
     ]
    }
   ],
   "source": [
    "## view topics\n",
    "topics = petitions_ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.453937), (16, 0.5032059)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this provides distrubution for the 89th document \n",
    "petitions_ldamodel[petitions_corpus[89]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 205: Create LDA model for debates data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## created corpus for lda based on uiversal dictionary \n",
    "tscript_corpus = [dictionary.doc2bow(text) for text in tscript_txt]\n",
    "pickle.dump(tscript_corpus, open('tscript_corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1512.2018258571625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## model creation \n",
    "# 25 minutes\n",
    "numTopics = 20\n",
    "tscript_ldamodel = LdaMulticore(tscript_corpus, num_topics = numTopics, id2word=dictionary, passes=10)\n",
    "tscript_ldamodel.save('tscript_model.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.012*\"right\" + 0.012*\"court\" + 0.012*\"justice\" + 0.011*\"prison\" + 0.009*\"government\"')\n",
      "(1, '0.023*\"business\" + 0.020*\"government\" + 0.009*\"people\" + 0.008*\"economy\" + 0.008*\"friend\"')\n",
      "(2, '0.027*\"ireland\" + 0.025*\"northern\" + 0.013*\"minister\" + 0.012*\"student\" + 0.011*\"university\"')\n",
      "(3, '0.012*\"government\" + 0.011*\"member\" + 0.009*\"local\" + 0.009*\"north\" + 0.008*\"wale\"')\n",
      "(4, '0.020*\"health\" + 0.016*\"service\" + 0.012*\"hospital\" + 0.012*\"patient\" + 0.009*\"people\"')\n",
      "(5, '0.013*\"right\" + 0.010*\"country\" + 0.010*\"people\" + 0.010*\"government\" + 0.008*\"member\"')\n",
      "(6, '0.029*\"people\" + 0.018*\"government\" + 0.011*\"benefit\" + 0.010*\"credit\" + 0.010*\"minister\"')\n",
      "(7, '0.022*\"child\" + 0.016*\"woman\" + 0.012*\"member\" + 0.008*\"people\" + 0.008*\"issue\"')\n",
      "(8, '0.020*\"government\" + 0.015*\"pension\" + 0.012*\"would\" + 0.011*\"budget\" + 0.010*\"people\"')\n",
      "(9, '0.019*\"european\" + 0.014*\"union\" + 0.013*\"government\" + 0.012*\"right\" + 0.012*\"minister\"')\n",
      "(10, '0.016*\"defence\" + 0.015*\"force\" + 0.010*\"armed\" + 0.008*\"friend\" + 0.008*\"member\"')\n",
      "(11, '0.021*\"house\" + 0.020*\"member\" + 0.015*\"would\" + 0.013*\"government\" + 0.010*\"committee\"')\n",
      "(12, '0.027*\"people\" + 0.013*\"member\" + 0.010*\"health\" + 0.008*\"debate\" + 0.008*\"support\"')\n",
      "(13, '0.027*\"local\" + 0.017*\"housing\" + 0.016*\"council\" + 0.016*\"government\" + 0.014*\"authority\"')\n",
      "(14, '0.009*\"would\" + 0.008*\"friend\" + 0.008*\"member\" + 0.007*\"government\" + 0.007*\"minister\"')\n",
      "(15, '0.016*\"energy\" + 0.012*\"government\" + 0.009*\"industry\" + 0.007*\"minister\" + 0.007*\"member\"')\n",
      "(16, '0.040*\"school\" + 0.016*\"education\" + 0.016*\"child\" + 0.009*\"government\" + 0.009*\"young\"')\n",
      "(17, '0.035*\"police\" + 0.014*\"crime\" + 0.010*\"officer\" + 0.009*\"people\" + 0.008*\"member\"')\n",
      "(18, '0.012*\"government\" + 0.011*\"member\" + 0.010*\"would\" + 0.009*\"financial\" + 0.009*\"company\"')\n",
      "(19, '0.022*\"amendment\" + 0.019*\"clause\" + 0.015*\"would\" + 0.009*\"committee\" + 0.007*\"section\"')\n"
     ]
    }
   ],
   "source": [
    "## view topics\n",
    "\n",
    "topics = tscript_ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
